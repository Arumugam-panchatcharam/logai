{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial: Log Clustering using LogAI\n",
    "\n",
    "This is an example to show how to use LogAI to conduct log clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data\n",
    "\n",
    "You can use `OpensetDataLoader` to load a sample open log dataset. Here we use HDFS dataset from\n",
    "[LogHub](https://zenodo.org/record/3227177#.Y1M3LezML0o) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:45.104457Z",
     "iopub.status.busy": "2023-01-25T05:07:45.103898Z",
     "iopub.status.idle": "2023-01-25T05:07:45.739727Z",
     "shell.execute_reply": "2023-01-25T05:07:45.738768Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/runner/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_configs/hdfs.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mpath to HDFS data}/HDFS_2000.log\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# modify to point to the HDFS_2000.log dataset.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDFS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mOpenSetDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOpenSetDataLoaderConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m logrecord \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     15\u001b[0m logrecord\u001b[38;5;241m.\u001b[39mto_dataframe()\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_data_loader.py:39\u001b[0m, in \u001b[0;36mOpenSetDataLoader.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: OpenSetDataLoaderConfig):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dl_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dl_config)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_data_loader.py:25\u001b[0m, in \u001b[0;36mget_config\u001b[0;34m(dataset_name, filepath)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mRetrieve the configuration of open log datasets to load data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m:param dataset_name: supported log dataset name from (\"hdfs\", \"bgl\", \"HealthApp\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m:param filepath: log file path\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m:return: DataLoaderConfig: the configuration to load open log datasets\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m config_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenset_configs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset_name\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     26\u001b[0m     config \u001b[38;5;241m=\u001b[39m DataLoaderConfig\u001b[38;5;241m.\u001b[39mfrom_dict(json\u001b[38;5;241m.\u001b[39mload(f))\n\u001b[1;32m     27\u001b[0m config\u001b[38;5;241m.\u001b[39mfilepath \u001b[38;5;241m=\u001b[39m filepath\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/runner/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_configs/hdfs.json'"
     ]
    }
   ],
   "source": [
    "from logai.dataloader.openset_data_loader import OpenSetDataLoader, OpenSetDataLoaderConfig\n",
    "\n",
    "#File Configuration\n",
    "filepath = \"{path to HDFS data}/HDFS_2000.log\" # modify to point to the HDFS_2000.log dataset.\n",
    "\n",
    "dataset_name = \"HDFS\"\n",
    "data_loader = OpenSetDataLoader(\n",
    "    OpenSetDataLoaderConfig(\n",
    "        dataset_name=dataset_name,\n",
    "        filepath=filepath)\n",
    ")\n",
    "\n",
    "logrecord = data_loader.load_data()\n",
    "\n",
    "logrecord.to_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "In preprocessing step user can retrieve and replace any regex strings and clean the raw loglines. This\n",
    "can be very useful to improve information extraction of the unstructured part of logs,\n",
    " as well as generate more structured attributes with domain knowledge.\n",
    "\n",
    "Here in the example, we use the below regex to retrieve Block IDs, IP addresses and filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:45.743652Z",
     "iopub.status.busy": "2023-01-25T05:07:45.743351Z",
     "iopub.status.idle": "2023-01-25T05:07:47.699958Z",
     "shell.execute_reply": "2023-01-25T05:07:47.698940Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logrecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreprocessorConfig, Preprocessor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m----> 4\u001b[0m loglines \u001b[38;5;241m=\u001b[39m \u001b[43mlogrecord\u001b[49m\u001b[38;5;241m.\u001b[39mbody[constants\u001b[38;5;241m.\u001b[39mLOGLINE_NAME]\n\u001b[1;32m      5\u001b[0m attributes \u001b[38;5;241m=\u001b[39m logrecord\u001b[38;5;241m.\u001b[39mattributes\n\u001b[1;32m      7\u001b[0m preprocessor_config \u001b[38;5;241m=\u001b[39m PreprocessorConfig(\n\u001b[1;32m      8\u001b[0m     custom_replace_list\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m         [\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?<=blk_)[-\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md]+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<block_id>\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logrecord' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from logai.preprocess.preprocessor import PreprocessorConfig, Preprocessor\n",
    "from logai.utils import constants\n",
    "\n",
    "loglines = logrecord.body[constants.LOGLINE_NAME]\n",
    "attributes = logrecord.attributes\n",
    "\n",
    "preprocessor_config = PreprocessorConfig(\n",
    "    custom_replace_list=[\n",
    "        [r\"(?<=blk_)[-\\d]+\", \"<block_id>\"],\n",
    "        [r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", \"<IP>\"],\n",
    "        [r\"(/[-\\w]+)+\", \"<file_path>\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = Preprocessor(preprocessor_config)\n",
    "\n",
    "clean_logs, custom_patterns = preprocessor.clean_log(\n",
    "    loglines\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "After preprocessing, we call auto-parsing algorithms to automatically parse the cleaned logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:47.704711Z",
     "iopub.status.busy": "2023-01-25T05:07:47.704086Z",
     "iopub.status.idle": "2023-01-25T05:07:47.733945Z",
     "shell.execute_reply": "2023-01-25T05:07:47.732954Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m log_parser_config \u001b[38;5;241m=\u001b[39m LogParserConfig(\n\u001b[1;32m     10\u001b[0m     parsing_algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     parsing_algo_params\u001b[38;5;241m=\u001b[39mparsing_algo_params\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m parser \u001b[38;5;241m=\u001b[39m LogParser(log_parser_config)\n\u001b[0;32m---> 15\u001b[0m parsed_result \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(\u001b[43mclean_logs\u001b[49m)\n\u001b[1;32m     17\u001b[0m parsed_loglines \u001b[38;5;241m=\u001b[39m parsed_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_logline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_logs' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainParams\n",
    "\n",
    "# parsing\n",
    "parsing_algo_params = DrainParams(\n",
    "    sim_th=0.5, depth=5\n",
    ")\n",
    "\n",
    "log_parser_config = LogParserConfig(\n",
    "    parsing_algorithm=\"drain\",\n",
    "    parsing_algo_params=parsing_algo_params\n",
    ")\n",
    "\n",
    "parser = LogParser(log_parser_config)\n",
    "parsed_result = parser.parse(clean_logs)\n",
    "\n",
    "parsed_loglines = parsed_result['parsed_logline']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Information Extraction\n",
    "\n",
    "### Vectorization for unstructured loglines\n",
    "\n",
    "Here we use `word2vec` to vectorize unstructured part of the logs. The output will be a list of\n",
    "numeric vectors that representing the semantic features of these log templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:47.738080Z",
     "iopub.status.busy": "2023-01-25T05:07:47.737561Z",
     "iopub.status.idle": "2023-01-25T05:07:49.071752Z",
     "shell.execute_reply": "2023-01-25T05:07:49.070768Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_loglines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m LogVectorizer(\n\u001b[1;32m      8\u001b[0m     vectorizer_config\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train vectorizer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m vectorizer\u001b[38;5;241m.\u001b[39mfit(\u001b[43mparsed_loglines\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Transform the loglines into features\u001b[39;00m\n\u001b[1;32m     15\u001b[0m log_vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(parsed_loglines)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_loglines' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.log_vectorizer import VectorizerConfig, LogVectorizer\n",
    "\n",
    "vectorizer_config = VectorizerConfig(\n",
    "    algo_name = \"word2vec\"\n",
    ")\n",
    "\n",
    "vectorizer = LogVectorizer(\n",
    "    vectorizer_config\n",
    ")\n",
    "\n",
    "# Train vectorizer\n",
    "vectorizer.fit(parsed_loglines)\n",
    "\n",
    "# Transform the loglines into features\n",
    "log_vectors = vectorizer.transform(parsed_loglines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Categorical Encoding for log attributes\n",
    "\n",
    "We also do categorical encoding for log attributes to convert the strings into numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:49.085924Z",
     "iopub.status.busy": "2023-01-25T05:07:49.085562Z",
     "iopub.status.idle": "2023-01-25T05:07:49.108147Z",
     "shell.execute_reply": "2023-01-25T05:07:49.107074Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attributes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m encoder_config \u001b[38;5;241m=\u001b[39m CategoricalEncoderConfig(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m CategoricalEncoder(encoder_config)\n\u001b[0;32m----> 7\u001b[0m attributes_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mattributes\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributes' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.categorical_encoder import CategoricalEncoderConfig, CategoricalEncoder\n",
    "\n",
    "encoder_config = CategoricalEncoderConfig(name=\"label_encoder\")\n",
    "\n",
    "encoder = CategoricalEncoder(encoder_config)\n",
    "\n",
    "attributes_encoded = encoder.fit_transform(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Then we extract and concate the semantic features for both the unstructured and structured part of logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:49.112154Z",
     "iopub.status.busy": "2023-01-25T05:07:49.111601Z",
     "iopub.status.idle": "2023-01-25T05:07:49.134893Z",
     "shell.execute_reply": "2023-01-25T05:07:49.133891Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logrecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minformation_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extractor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractorConfig, FeatureExtractor\n\u001b[0;32m----> 3\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mlogrecord\u001b[49m\u001b[38;5;241m.\u001b[39mtimestamp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m FeatureExtractorConfig(\n\u001b[1;32m      6\u001b[0m     max_feature_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m FeatureExtractor(config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logrecord' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.feature_extractor import FeatureExtractorConfig, FeatureExtractor\n",
    "\n",
    "timestamps = logrecord.timestamp['timestamp']\n",
    "\n",
    "config = FeatureExtractorConfig(\n",
    "    max_feature_len=100\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "_, feature_vector = feature_extractor.convert_to_feature_vector(log_vectors, attributes_encoded, timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "Here we use K-Means clustering algorithm as an example. WE set the number of clusters to 7 in\n",
    "K-Means algorithm parameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:49.138874Z",
     "iopub.status.busy": "2023-01-25T05:07:49.138410Z",
     "iopub.status.idle": "2023-01-25T05:07:49.218940Z",
     "shell.execute_reply": "2023-01-25T05:07:49.217728Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m clustering_config \u001b[38;5;241m=\u001b[39m ClusteringConfig(\n\u001b[1;32m      5\u001b[0m     algo_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkmeans\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     algo_params\u001b[38;5;241m=\u001b[39mKMeansParams(\n\u001b[1;32m      7\u001b[0m         n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m log_clustering \u001b[38;5;241m=\u001b[39m Clustering(clustering_config)\n\u001b[0;32m---> 13\u001b[0m log_clustering\u001b[38;5;241m.\u001b[39mfit(\u001b[43mfeature_vector\u001b[49m)\n\u001b[1;32m     15\u001b[0m cluster_id \u001b[38;5;241m=\u001b[39m log_clustering\u001b[38;5;241m.\u001b[39mpredict(feature_vector)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_vector' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.algorithms.clustering_algo.kmeans import KMeansParams\n",
    "from logai.analysis.clustering import ClusteringConfig, Clustering\n",
    "\n",
    "clustering_config = ClusteringConfig(\n",
    "    algo_name='kmeans',\n",
    "    algo_params=KMeansParams(\n",
    "        n_clusters=7\n",
    "    )\n",
    ")\n",
    "\n",
    "log_clustering = Clustering(clustering_config)\n",
    "\n",
    "log_clustering.fit(feature_vector)\n",
    "\n",
    "cluster_id = log_clustering.predict(feature_vector).astype(str).rename('cluster_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-25T05:07:49.222819Z",
     "iopub.status.busy": "2023-01-25T05:07:49.222519Z",
     "iopub.status.idle": "2023-01-25T05:07:49.241199Z",
     "shell.execute_reply": "2023-01-25T05:07:49.240169Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logrecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check clustering results.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlogrecord\u001b[49m\u001b[38;5;241m.\u001b[39mto_dataframe()\u001b[38;5;241m.\u001b[39mjoin(cluster_id)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logrecord' is not defined"
     ]
    }
   ],
   "source": [
    "# Check clustering results.\n",
    "logrecord.to_dataframe().join(cluster_id).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
