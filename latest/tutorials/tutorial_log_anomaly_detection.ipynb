{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial: Log Anomaly Detection Using LogAI\n",
    "\n",
    "This is an example to show how to use LogAI to conduct log anomaly detection analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data\n",
    "\n",
    "You can use `OpensetDataLoader` to load a sample open log dataset. Here we use HealthApp dataset from\n",
    "[LogHub](https://zenodo.org/record/3227177#.Y1M3LezML0o) as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:29.240523Z",
     "iopub.status.busy": "2023-01-26T17:42:29.240052Z",
     "iopub.status.idle": "2023-01-26T17:42:29.824801Z",
     "shell.execute_reply": "2023-01-26T17:42:29.823248Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/runner/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_configs/healthapp.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHealthApp_2000.log\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Point to the target HealthApp.log dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHealthApp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mOpenSetDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOpenSetDataLoaderConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m logrecord \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     17\u001b[0m logrecord\u001b[38;5;241m.\u001b[39mto_dataframe()\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_data_loader.py:39\u001b[0m, in \u001b[0;36mOpenSetDataLoader.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: OpenSetDataLoaderConfig):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dl_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dl_config)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_data_loader.py:25\u001b[0m, in \u001b[0;36mget_config\u001b[0;34m(dataset_name, filepath)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mRetrieve the configuration of open log datasets to load data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m:param dataset_name: supported log dataset name from (\"hdfs\", \"bgl\", \"HealthApp\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m:param filepath: log file path\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m:return: DataLoaderConfig: the configuration to load open log datasets\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m config_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenset_configs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset_name\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     26\u001b[0m     config \u001b[38;5;241m=\u001b[39m DataLoaderConfig\u001b[38;5;241m.\u001b[39mfrom_dict(json\u001b[38;5;241m.\u001b[39mload(f))\n\u001b[1;32m     27\u001b[0m config\u001b[38;5;241m.\u001b[39mfilepath \u001b[38;5;241m=\u001b[39m filepath\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/runner/work/logai/logai/venv/lib/python3.10/site-packages/logai/dataloader/openset_configs/healthapp.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from logai.dataloader.openset_data_loader import OpenSetDataLoader, OpenSetDataLoaderConfig\n",
    "\n",
    "#File Configuration\n",
    "filepath = os.path.join(\"..\", \"datasets\", \"HealthApp_2000.log\") # Point to the target HealthApp.log dataset\n",
    "\n",
    "dataset_name = \"HealthApp\"\n",
    "data_loader = OpenSetDataLoader(\n",
    "    OpenSetDataLoaderConfig(\n",
    "        dataset_name=dataset_name,\n",
    "        filepath=filepath)\n",
    ")\n",
    "\n",
    "logrecord = data_loader.load_data()\n",
    "\n",
    "logrecord.to_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "In preprocessing step user can retrieve and replace any regex strings and clean the raw loglines. This\n",
    "can be very useful to improve information extraction of the unstructured part of logs,\n",
    " as well as generate more structured attributes with domain knowledge.\n",
    "\n",
    "Here in the example, we use the below regex to retrieve IP addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:29.860461Z",
     "iopub.status.busy": "2023-01-26T17:42:29.859980Z",
     "iopub.status.idle": "2023-01-26T17:42:31.477322Z",
     "shell.execute_reply": "2023-01-26T17:42:31.476555Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logrecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreprocessorConfig, Preprocessor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m----> 4\u001b[0m loglines \u001b[38;5;241m=\u001b[39m \u001b[43mlogrecord\u001b[49m\u001b[38;5;241m.\u001b[39mbody[constants\u001b[38;5;241m.\u001b[39mLOGLINE_NAME]\n\u001b[1;32m      5\u001b[0m attributes \u001b[38;5;241m=\u001b[39m logrecord\u001b[38;5;241m.\u001b[39mattributes\n\u001b[1;32m      7\u001b[0m preprocessor_config \u001b[38;5;241m=\u001b[39m PreprocessorConfig(\n\u001b[1;32m      8\u001b[0m     custom_replace_list\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m         [\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<IP>\u001b[39m\u001b[38;5;124m\"\u001b[39m],   \u001b[38;5;66;03m# retrieve all IP addresses and replace with <IP> tag in the original string.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logrecord' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.preprocess.preprocessor import PreprocessorConfig, Preprocessor\n",
    "from logai.utils import constants\n",
    "\n",
    "loglines = logrecord.body[constants.LOGLINE_NAME]\n",
    "attributes = logrecord.attributes\n",
    "\n",
    "preprocessor_config = PreprocessorConfig(\n",
    "    custom_replace_list=[\n",
    "        [r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", \"<IP>\"],   # retrieve all IP addresses and replace with <IP> tag in the original string.\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = Preprocessor(preprocessor_config)\n",
    "\n",
    "clean_logs, custom_patterns = preprocessor.clean_log(\n",
    "    loglines\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "After preprocessing, we call auto-parsing algorithms to automatically parse the cleaned logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:31.480538Z",
     "iopub.status.busy": "2023-01-26T17:42:31.480110Z",
     "iopub.status.idle": "2023-01-26T17:42:31.504706Z",
     "shell.execute_reply": "2023-01-26T17:42:31.503971Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m log_parser_config \u001b[38;5;241m=\u001b[39m LogParserConfig(\n\u001b[1;32m     10\u001b[0m     parsing_algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     parsing_algo_params\u001b[38;5;241m=\u001b[39mparsing_algo_params\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m parser \u001b[38;5;241m=\u001b[39m LogParser(log_parser_config)\n\u001b[0;32m---> 15\u001b[0m parsed_result \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(\u001b[43mclean_logs\u001b[49m)\n\u001b[1;32m     17\u001b[0m parsed_loglines \u001b[38;5;241m=\u001b[39m parsed_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_logline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_logs' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainParams\n",
    "\n",
    "# parsing\n",
    "parsing_algo_params = DrainParams(\n",
    "    sim_th=0.5, depth=5\n",
    ")\n",
    "\n",
    "log_parser_config = LogParserConfig(\n",
    "    parsing_algorithm=\"drain\",\n",
    "    parsing_algo_params=parsing_algo_params\n",
    ")\n",
    "\n",
    "parser = LogParser(log_parser_config)\n",
    "parsed_result = parser.parse(clean_logs)\n",
    "\n",
    "parsed_loglines = parsed_result['parsed_logline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Time-series Anomaly Detection\n",
    "\n",
    "Here we show an example to conduct time-series anomaly detection with parsed logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "After parsing the logs and get log templates, we can extract timeseries features by coverting\n",
    "these parsed loglines into counter vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:31.507826Z",
     "iopub.status.busy": "2023-01-26T17:42:31.507277Z",
     "iopub.status.idle": "2023-01-26T17:42:31.805988Z",
     "shell.execute_reply": "2023-01-26T17:42:31.805293Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logrecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m FeatureExtractorConfig(\n\u001b[1;32m      4\u001b[0m     group_by_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m15min\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     group_by_category\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_logline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m FeatureExtractor(config)\n\u001b[0;32m---> 10\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mlogrecord\u001b[49m\u001b[38;5;241m.\u001b[39mtimestamp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m parsed_loglines \u001b[38;5;241m=\u001b[39m parsed_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_logline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m counter_vector \u001b[38;5;241m=\u001b[39m feature_extractor\u001b[38;5;241m.\u001b[39mconvert_to_counter_vector(\n\u001b[1;32m     13\u001b[0m     log_pattern\u001b[38;5;241m=\u001b[39mparsed_loglines,\n\u001b[1;32m     14\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mattributes,\n\u001b[1;32m     15\u001b[0m     timestamps\u001b[38;5;241m=\u001b[39mtimestamps\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logrecord' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.feature_extractor import FeatureExtractorConfig, FeatureExtractor\n",
    "\n",
    "config = FeatureExtractorConfig(\n",
    "    group_by_time=\"15min\",\n",
    "    group_by_category=['parsed_logline', 'Action', 'ID'],\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "timestamps = logrecord.timestamp['timestamp']\n",
    "parsed_loglines = parsed_result['parsed_logline']\n",
    "counter_vector = feature_extractor.convert_to_counter_vector(\n",
    "    log_pattern=parsed_loglines,\n",
    "    attributes=attributes,\n",
    "    timestamps=timestamps\n",
    ")\n",
    "\n",
    "counter_vector.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "With the generated `counter_vcetor`, you can use `AnomalyDetector` to detect timeseries anomalies.\n",
    "Here we use an algorithm in Merlion library called `DynamicBaseLine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:31.809265Z",
     "iopub.status.busy": "2023-01-26T17:42:31.809029Z",
     "iopub.status.idle": "2023-01-26T17:42:33.392652Z",
     "shell.execute_reply": "2023-01-26T17:42:33.391914Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counter_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m counter_vector[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcounter_vector\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m      6\u001b[0m                 [\n\u001b[1;32m      7\u001b[0m                     constants\u001b[38;5;241m.\u001b[39mLOG_COUNTS,\n\u001b[1;32m      8\u001b[0m                     constants\u001b[38;5;241m.\u001b[39mLOG_TIMESTAMPS,\n\u001b[1;32m      9\u001b[0m                     constants\u001b[38;5;241m.\u001b[39mEVENT_INDEX\n\u001b[1;32m     10\u001b[0m                 ],\n\u001b[1;32m     11\u001b[0m                 axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m             )\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     13\u001b[0m                 \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m             )\n\u001b[1;32m     16\u001b[0m attr_list \u001b[38;5;241m=\u001b[39m counter_vector[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     18\u001b[0m anomaly_detection_config \u001b[38;5;241m=\u001b[39m AnomalyDetectionConfig(\n\u001b[1;32m     19\u001b[0m     algo_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counter_vector' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.analysis.anomaly_detector import AnomalyDetector, AnomalyDetectionConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "counter_vector[\"attribute\"] = counter_vector.drop(\n",
    "                [\n",
    "                    constants.LOG_COUNTS,\n",
    "                    constants.LOG_TIMESTAMPS,\n",
    "                    constants.EVENT_INDEX\n",
    "                ],\n",
    "                axis=1\n",
    "            ).apply(\n",
    "                lambda x: \"-\".join(x.astype(str)), axis=1\n",
    "            )\n",
    "\n",
    "attr_list = counter_vector[\"attribute\"].unique()\n",
    "\n",
    "anomaly_detection_config = AnomalyDetectionConfig(\n",
    "    algo_name='dbl'\n",
    ")\n",
    "\n",
    "res = pd.DataFrame()\n",
    "for attr in attr_list:\n",
    "    temp_df = counter_vector[counter_vector[\"attribute\"] == attr]\n",
    "    if temp_df.shape[0] >= constants.MIN_TS_LENGTH:\n",
    "        train, test = train_test_split(\n",
    "            temp_df[[constants.LOG_TIMESTAMPS, constants.LOG_COUNTS]],\n",
    "            shuffle=False,\n",
    "            train_size=0.3\n",
    "        )\n",
    "        anomaly_detector = AnomalyDetector(anomaly_detection_config)\n",
    "        anomaly_detector.fit(train)\n",
    "        anom_score = anomaly_detector.predict(test)\n",
    "        res = res.append(anom_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:33.395608Z",
     "iopub.status.busy": "2023-01-26T17:42:33.395238Z",
     "iopub.status.idle": "2023-01-26T17:42:33.409680Z",
     "shell.execute_reply": "2023-01-26T17:42:33.409111Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counter_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get anomalous datapoints\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m anomalies \u001b[38;5;241m=\u001b[39m \u001b[43mcounter_vector\u001b[49m\u001b[38;5;241m.\u001b[39miloc[res[res\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mindex]\n\u001b[1;32m      3\u001b[0m anomalies\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counter_vector' is not defined"
     ]
    }
   ],
   "source": [
    "# Get anomalous datapoints\n",
    "anomalies = counter_vector.iloc[res[res>0].index]\n",
    "anomalies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Semantic Anomaly Detection\n",
    "\n",
    "We can also use the log template for semantic based anomaly detection. In this approach, we retrieve\n",
    "the semantic features from the logs. This includes two parts: vectorizing the unstructured log templates\n",
    "and encoding the structured log attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Vectorization for unstructured loglines\n",
    "\n",
    "Here we use `word2vec` to vectorize unstructured part of the logs. The output will be a list of\n",
    "numeric vectors that representing the semantic features of these log templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:33.412737Z",
     "iopub.status.busy": "2023-01-26T17:42:33.412168Z",
     "iopub.status.idle": "2023-01-26T17:42:33.429188Z",
     "shell.execute_reply": "2023-01-26T17:42:33.428622Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_loglines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m LogVectorizer(\n\u001b[1;32m      8\u001b[0m     vectorizer_config\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train vectorizer\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m vectorizer\u001b[38;5;241m.\u001b[39mfit(\u001b[43mparsed_loglines\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Transform the loglines into features\u001b[39;00m\n\u001b[1;32m     15\u001b[0m log_vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(parsed_loglines)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_loglines' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.log_vectorizer import VectorizerConfig, LogVectorizer\n",
    "\n",
    "vectorizer_config = VectorizerConfig(\n",
    "    algo_name = \"word2vec\"\n",
    ")\n",
    "\n",
    "vectorizer = LogVectorizer(\n",
    "    vectorizer_config\n",
    ")\n",
    "\n",
    "# Train vectorizer\n",
    "vectorizer.fit(parsed_loglines)\n",
    "\n",
    "# Transform the loglines into features\n",
    "log_vectors = vectorizer.transform(parsed_loglines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Categorical Encoding for log attributes\n",
    "\n",
    "We also do categorical encoding for log attributes to convert the strings into numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:33.432494Z",
     "iopub.status.busy": "2023-01-26T17:42:33.431542Z",
     "iopub.status.idle": "2023-01-26T17:42:33.448516Z",
     "shell.execute_reply": "2023-01-26T17:42:33.447899Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attributes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m encoder_config \u001b[38;5;241m=\u001b[39m CategoricalEncoderConfig(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m CategoricalEncoder(encoder_config)\n\u001b[0;32m----> 7\u001b[0m attributes_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mattributes\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributes' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.categorical_encoder import CategoricalEncoderConfig, CategoricalEncoder\n",
    "\n",
    "encoder_config = CategoricalEncoderConfig(name=\"label_encoder\")\n",
    "\n",
    "encoder = CategoricalEncoder(encoder_config)\n",
    "\n",
    "attributes_encoded = encoder.fit_transform(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Then we extract and concate the semantic features for both the unstructured and structured part of logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:33.451081Z",
     "iopub.status.busy": "2023-01-26T17:42:33.450682Z",
     "iopub.status.idle": "2023-01-26T17:42:33.466239Z",
     "shell.execute_reply": "2023-01-26T17:42:33.465246Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logrecord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minformation_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extractor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractorConfig, FeatureExtractor\n\u001b[0;32m----> 3\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mlogrecord\u001b[49m\u001b[38;5;241m.\u001b[39mtimestamp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m FeatureExtractorConfig(\n\u001b[1;32m      6\u001b[0m     max_feature_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m FeatureExtractor(config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logrecord' is not defined"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.feature_extractor import FeatureExtractorConfig, FeatureExtractor\n",
    "\n",
    "timestamps = logrecord.timestamp['timestamp']\n",
    "\n",
    "config = FeatureExtractorConfig(\n",
    "    max_feature_len=100\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "_, feature_vector = feature_extractor.convert_to_feature_vector(log_vectors, attributes_encoded, timestamps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "With the extracted log semantic feature set, we can perform anomaly detection to find the abnormal\n",
    "logs. Here we use `isolation_forest` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:33.469218Z",
     "iopub.status.busy": "2023-01-26T17:42:33.468701Z",
     "iopub.status.idle": "2023-01-26T17:42:33.487882Z",
     "shell.execute_reply": "2023-01-26T17:42:33.486757Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m train, test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mfeature_vector\u001b[49m, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manomaly_detection_algo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misolation_forest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IsolationForestParams\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manomaly_detector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnomalyDetectionConfig, AnomalyDetector\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_vector' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(feature_vector, train_size=0.7, test_size=0.3)\n",
    "\n",
    "from logai.algorithms.anomaly_detection_algo.isolation_forest import IsolationForestParams\n",
    "from logai.analysis.anomaly_detector import AnomalyDetectionConfig, AnomalyDetector\n",
    "\n",
    "algo_params = IsolationForestParams(\n",
    "    n_estimators=10,\n",
    "    max_features=100\n",
    ")\n",
    "config = AnomalyDetectionConfig(\n",
    "    algo_name='isolation_forest',\n",
    "    algo_params=algo_params\n",
    ")\n",
    "\n",
    "anomaly_detector = AnomalyDetector(config)\n",
    "anomaly_detector.fit(train)\n",
    "res = anomaly_detector.predict(test)\n",
    "# obtain the anomalous datapoints\n",
    "anomalies = res[res==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check the corresponding loglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:33.490671Z",
     "iopub.status.busy": "2023-01-26T17:42:33.490341Z",
     "iopub.status.idle": "2023-01-26T17:42:33.505138Z",
     "shell.execute_reply": "2023-01-26T17:42:33.504557Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loglines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloglines\u001b[49m\u001b[38;5;241m.\u001b[39miloc[anomalies\u001b[38;5;241m.\u001b[39mindex]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loglines' is not defined"
     ]
    }
   ],
   "source": [
    "loglines.iloc[anomalies.index].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Check the corresponding attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-01-26T17:42:33.508002Z",
     "iopub.status.busy": "2023-01-26T17:42:33.507547Z",
     "iopub.status.idle": "2023-01-26T17:42:33.523083Z",
     "shell.execute_reply": "2023-01-26T17:42:33.522000Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attributes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattributes\u001b[49m\u001b[38;5;241m.\u001b[39miloc[anomalies\u001b[38;5;241m.\u001b[39mindex]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributes' is not defined"
     ]
    }
   ],
   "source": [
    "attributes.iloc[anomalies.index].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
