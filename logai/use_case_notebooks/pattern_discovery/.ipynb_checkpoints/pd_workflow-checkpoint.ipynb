{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from os import listdir\n",
    "from os.path import isfile, join, exists, dirname\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.dataloader.data_loader import FileConfig\n",
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainParams\n",
    "from logai.preprocess.preprocess import Preprocessor, PreprocessorConfig\n",
    "from logai.dataloader.data_model import LogRecordObject\n",
    "from logai.utils import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# const\n",
    "\n",
    "\n",
    "SIM_THRE = 0.6\n",
    "\n",
    "# functions\n",
    "\n",
    "def read_log(filepath):\n",
    "    name = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        loglines = []\n",
    "        cluster_labels = []\n",
    "        pattern = \",{}`\".format(name)\n",
    "        for l in f.readlines():\n",
    "            start_index = l.find(\",{}`\".format(name))\n",
    "            if start_index != -1:\n",
    "                logline = l[start_index:-1].replace(pattern, \"\")\n",
    "                if logline:\n",
    "                    loglines.append(logline.strip())\n",
    "                    cluster_labels.append(l.split(\",\")[0])\n",
    "    f.close()\n",
    "    res = pd.DataFrame(list(zip(loglines, cluster_labels)),\n",
    "                     columns =[\"logline\", \"cluster_label\"])\n",
    "    return res\n",
    "\n",
    "# read new ailtn\n",
    "def read_new_logs(path):\n",
    "    df = pd.read_csv(path, header=0, low_memory=False)\n",
    "    res = df[['_raw', 'cluster_label']]\n",
    "    res.columns = ['logline', 'cluster_label']\n",
    "    return res\n",
    "\n",
    "\n",
    "# Levenshtein distance\n",
    "import numpy as np\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "\n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if token1[t1-1] == token2[t2-1] \\\n",
    "                    or token1[t1-1] == \"*\" or token2[t2-1] == \"*\":\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "\n",
    "                if a <= b and a <= c:\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif b <= a and b <= c:\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(token1)][len(token2)]\n",
    "\n",
    "def calc_distance(base, p):\n",
    "    b_token = base.split(\" \")\n",
    "    p_token = p.split(\" \")\n",
    "\n",
    "    l = max(len(b_token), len(p_token))\n",
    "    dis = levenshteinDistanceDP(b_token, p_token)\n",
    "    sim = (l - dis) * 1.0 / l\n",
    "    return dis, sim\n",
    "\n",
    "def get_sim_table(parsed_loglines: pd.Series, lrt, cluster):\n",
    "    pattern_counts = parsed_loglines.value_counts(normalize=True, sort=True, ascending=False).reset_index()\n",
    "    pattern_counts.columns = [\"pattern\", \"portion\"]\n",
    "    pattern_counts.sort_values(by=\"portion\", ascending=False, inplace=True)\n",
    "    base_pattern = pattern_counts[\"pattern\"][0]\n",
    "\n",
    "    similarity_table = []\n",
    "    for index, row in pattern_counts.iterrows():\n",
    "        pattern = row[\"pattern\"]\n",
    "        portion = row[\"portion\"]\n",
    "        dis, sim = calc_distance(base_pattern, pattern)\n",
    "        similarity_table.append([lrt, cluster, portion, dis, sim, pattern, base_pattern])\n",
    "    res = pd.DataFrame.from_dict(similarity_table)\n",
    "    res.columns=[\"lrt\", \"cluster\", \"portion\", \"distance\", \"similarity\", \"pattern\", \"base_pattern\"]\n",
    "    return res, base_pattern\n",
    "\n",
    "from logai.utils.functions import get_parameter_list\n",
    "\n",
    "\n",
    "# read new ailtn\n",
    "def read_new_logs(path):\n",
    "    df = pd.read_csv(path, header=0, low_memory=False)\n",
    "    res = df[['_raw', 'cluster_label']]\n",
    "    res.columns = ['logline', 'cluster_label']\n",
    "    return res\n",
    "\n",
    "def parse_logs(loglines, cluster_label, lrt):\n",
    "    drain_config = DrainParams(sim_th=0.1,                       extra_delimiters=[])\n",
    "    log_parser_config = LogParserConfig(parsing_algorithm='drain',                              parsing_algo_params=drain_config)\n",
    "    parser = LogParser(log_parser_config)\n",
    "    parsed_result = parser.parse(loglines.dropna())\n",
    "    parsed_result['cluster_label'] = cluster_label\n",
    "    parsed_result['lrt'] = lrt\n",
    "    return parsed_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_parameter_list_2(row):\n",
    "    parameter_list = []\n",
    "    if not isinstance(row[constants.LOGLINE_NAME], str) or not isinstance(row[constants.PARSED_LOGLINE_NAME], str):\n",
    "        return parameter_list\n",
    "    ll = row[constants.LOGLINE_NAME].split()\n",
    "    pp = row[constants.PARSED_LOGLINE_NAME].split()\n",
    "    buffer = []\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    consec_pattern = False\n",
    "    while i < len(ll) and j < len(pp):\n",
    "        # print(ll[i], pp[j])\n",
    "        if ll[i] == pp[j]:\n",
    "            if buffer:\n",
    "                parameter_list.append(\" \".join(buffer))\n",
    "                buffer =[]\n",
    "            consec_pattern = False\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif pp[j] == \"*\":\n",
    "            if consec_pattern:\n",
    "                parameter_list.append(\" \".join(buffer))\n",
    "                buffer = [ll[i]]\n",
    "            else:\n",
    "                buffer.append(ll[i])\n",
    "            consec_pattern = True\n",
    "            i+=1\n",
    "            j+=1\n",
    "        else:\n",
    "            buffer.append(ll[i])\n",
    "            i += 1\n",
    "    if buffer:\n",
    "        if i < len(ll):\n",
    "            parameter_list.append(\" \".join(buffer + ll[i:]))\n",
    "        else:\n",
    "            parameter_list.append(\" \".join(buffer))\n",
    "    return parameter_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlmul\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/mlmul.csv\n",
      "ksgen\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/ksgen.csv\n",
      "augen\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/augen.csv\n",
      "s\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/s.csv\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "ffgen\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/ffgen.csv\n",
      "similarity is lower than threshold: 0.6\n",
      "phqry\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/phqry.csv\n",
      "similarity is lower than threshold: 0.6\n",
      "ppcmi\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/ppcmi.csv\n",
      "cptsk\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/cptsk.csv\n",
      "ailtn\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/ailtn_with_label.csv\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "A\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/A_with_label.csv\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "U\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/U_with_label.csv\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "mqfrm\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/mqfrm_with_label.csv\n",
      "mqdbg\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/mqdbg_with_label.csv\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n",
      "similarity is lower than threshold: 0.6\n"
     ]
    }
   ],
   "source": [
    "TARGET_CLUSTER = 13\n",
    "TARGET_LRT= 'ailtn'\n",
    "\n",
    "OLD_DIR = \"/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/\"\n",
    "DIR = \"/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/\"\n",
    "\n",
    "\n",
    "old_file_list = os.listdir(OLD_DIR)\n",
    "file_list = os.listdir(DIR)\n",
    "\n",
    "new_labels = [\"ailtn\", \"mqdbg\", \"mqfrm\"]\n",
    "\n",
    "old_path = list(filter(None, [os.path.join(OLD_DIR, f) if f.split(\".\")[0] not in new_labels else None for f in old_file_list]))\n",
    "new_path = [os.path.join(DIR, f) for f in file_list]\n",
    "\n",
    "paths = old_path + new_path\n",
    "\n",
    "paths\n",
    "\n",
    "\n",
    "parsing_res = []\n",
    "\n",
    "logline_map = pd.DataFrame()\n",
    "\n",
    "non_qualified_clusters = []\n",
    "\n",
    "for path in paths:\n",
    "    if \"sf_datasets_new_labels\" in path:\n",
    "        file = path.split(\"/\")[-1]\n",
    "        lrt = file.split('_')[0]\n",
    "    # if lrt != TARGET_LRT:\n",
    "    #     continue\n",
    "        print(lrt)\n",
    "        print(path)\n",
    "        logs = read_new_logs(path)\n",
    "    else:\n",
    "        file = path.split(\"/\")[-1]\n",
    "        lrt = file.split(\".\")[0]\n",
    "        print(lrt)\n",
    "        print(path)\n",
    "        logs = read_log(path)\n",
    "\n",
    "    logrecord = LogRecordObject(body=logs['logline'], attributes=logs[['cluster_label']])\n",
    "    similarity = pd.DataFrame()\n",
    "    custom_delimeter_regex = [r\"`+|\\s+\"]\n",
    "    preprocessor = Preprocessor(PreprocessorConfig(custom_delimiters_regex=custom_delimeter_regex))\n",
    "    preprocessed_loglines = preprocessor.clean_log(logrecord.body)\n",
    "    index_groups = preprocessor.group_log_index(logrecord.attributes, by=['cluster_label'])\n",
    "\n",
    "\n",
    "    for i in index_groups.index:\n",
    "        cluster_label = index_groups['cluster_label'].iloc[i]\n",
    "        # if cluster_label != TARGET_CLUSTER:\n",
    "        #     continue\n",
    "        indices = index_groups['group_index'][i]\n",
    "        if index_groups['cluster_label'].iloc[i] == -1:\n",
    "            continue\n",
    "        if len(indices) == 1:\n",
    "            continue\n",
    "        loglines_in_group = preprocessed_loglines.iloc[indices]\n",
    "        parsed_result = parse_logs(loglines_in_group, cluster_label, lrt)\n",
    "        logline_map = logline_map.append(parsed_result)\n",
    "        temp_res = parsed_result\n",
    "        uniq_patterns = parsed_result[constants.PARSED_LOGLINE_NAME].unique()\n",
    "        num_p = len(uniq_patterns)\n",
    "\n",
    "        if num_p > 1:\n",
    "            similarity_table, base_pattern = get_sim_table(parsed_result[constants.PARSED_LOGLINE_NAME], lrt, cluster_label)\n",
    "            if min(similarity_table['similarity']) < SIM_THRE:\n",
    "                print(\"similarity is lower than threshold: {}\".format(SIM_THRE))\n",
    "                non_qualified_clusters.append((lrt, cluster_label, min(similarity_table['similarity'])))\n",
    "\n",
    "        else:\n",
    "            base_pattern = uniq_patterns[0]\n",
    "\n",
    "        if \"*\" in base_pattern:\n",
    "            parsed_result[constants.PARSED_LOGLINE_NAME] = base_pattern\n",
    "            parsed_result[constants.PARAMETER_LIST_NAME] = parsed_result.apply(get_parameter_list_2, axis=1)\n",
    "\n",
    "        para_list = parsed_result[constants.PARAMETER_LIST_NAME]\n",
    "        para_list = list(map(set, itertools.zip_longest(*para_list, fillvalue=None)))\n",
    "        para_list = [set(r) for r in para_list]\n",
    "\n",
    "        ps_res = {\n",
    "            \"lrt\": lrt,\n",
    "            \"cluster_label\": cluster_label,\n",
    "            \"base_pattern\": base_pattern,\n",
    "            \"parameter_list\": para_list\n",
    "        }\n",
    "        parsing_res.append(ps_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Res_DIR = \"/Users/qcheng/workspace/gitsoma/logai/logai/results/pattern_discovery/\"\n",
    "\n",
    "logline_map.to_csv(os.path.join(Res_DIR, \"logline_map.csv\"))\n",
    "\n",
    "res_file = os.path.join(Res_DIR, \"cluster_patterns.json\")\n",
    "pd.DataFrame.from_dict(parsing_res).to_json(res_file, orient='records')\n",
    "pd.DataFrame(non_qualified_clusters, columns=[\"lrt\", \"cluster\", \"min_sim\"]).to_csv(os.path.join(Res_DIR, \"non_qualified_clusters.csv\"), index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
