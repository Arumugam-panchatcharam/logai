{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, exists, dirname\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.dataloader.data_loader import DataLoaderConfig\n",
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainParams\n",
    "from logai.preprocess.preprocess import Preprocessor, PreprocessorConfig\n",
    "from logai.dataloader.data_model import LogRecordObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def read_log(name):\n",
    "    filepath = os.path.join(DIR, \"{}.csv\".format(name))\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        loglines = []\n",
    "        cluster_labels = []\n",
    "        pattern = \",{}`\".format(name)\n",
    "        for l in f.readlines():\n",
    "            start_index = l.find(\",{}`\".format(name))\n",
    "            if start_index != -1:\n",
    "                logline = l[start_index:-1].replace(pattern, \"\")\n",
    "                if logline:\n",
    "                    loglines.append(logline.strip())\n",
    "                    cluster_labels.append(l.split(\",\")[0])\n",
    "    f.close()\n",
    "    return loglines, cluster_labels\n",
    "\n",
    "# read new ailtn\n",
    "def read_new_ailtn():\n",
    "    path = '/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/ailtn_with_label.csv'\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    return df['_raw'], df['cluster_label']\n",
    "\n",
    "# Levenshtein distance\n",
    "import numpy as np\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "\n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if token1[t1-1] == token2[t2-1] \\\n",
    "                    or token1[t1-1] == \"*\" or token2[t2-1] == \"*\":\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "\n",
    "                if a <= b and a <= c:\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif b <= a and b <= c:\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len(token1)][len(token2)]\n",
    "\n",
    "def calc_distance(base, p):\n",
    "    b_token = base.split(\" \")\n",
    "    p_token = p.split(\" \")\n",
    "\n",
    "    l = max(len(b_token), len(p_token))\n",
    "    dis = levenshteinDistanceDP(b_token, p_token)\n",
    "    sim = (l - dis) * 1.0 / l\n",
    "    return dis, sim\n",
    "\n",
    "def get_sim_table(parsed_loglines: pd.Series, lrt, cluster):\n",
    "    pattern_counts = parsed_loglines.value_counts(normalize=True, sort=True, ascending=False).reset_index()\n",
    "    pattern_counts.columns = [\"pattern\", \"portion\"]\n",
    "    pattern_counts.sort_values(by=\"portion\", ascending=False, inplace=True)\n",
    "    base_pattern = pattern_counts[\"pattern\"][0]\n",
    "\n",
    "    similarity_table = []\n",
    "    for index, row in pattern_counts.iterrows():\n",
    "        pattern = row[\"pattern\"]\n",
    "        portion = row[\"portion\"]\n",
    "        dis, sim = calc_distance(base_pattern, pattern)\n",
    "        similarity_table.append([lrt, cluster, portion, dis, sim, pattern, base_pattern])\n",
    "    res = pd.DataFrame.from_dict(similarity_table)\n",
    "    res.columns=[\"lrt\", \"cluster\", \"portion\", \"distance\", \"similarity\", \"pattern\", \"base_pattern\"]\n",
    "    return res, base_pattern\n",
    "\n",
    "from logai.utils.functions import get_parameter_list\n",
    "\n",
    "\n",
    "# read new ailtn\n",
    "def read_new_logs(path):\n",
    "    df = pd.read_csv(path, header=0, low_memory=False)\n",
    "    res = df[['_raw', 'cluster_label']]\n",
    "    res.columns = ['logline', 'cluster_label']\n",
    "    return res\n",
    "\n",
    "def parse_logs(loglines, cluster_label, lrt):\n",
    "    drain_config = DrainParams(sim_th=0.1,                       extra_delimiters=[])\n",
    "    log_parser_config = LogParserConfig(parsing_algorithm='drain',                              parsing_algo_params=drain_config)\n",
    "    parser = LogParser(log_parser_config)\n",
    "    parsed_result = parser.parse(loglines.dropna())\n",
    "    parsed_result['cluster_label'] = cluster_label\n",
    "    parsed_result['lrt'] = lrt\n",
    "    return parsed_result\n",
    "\n",
    "\n",
    "def cal_similarity_for_lrt(logs:pd.DataFrame, lrt):\n",
    "\n",
    "    logrecord = LogRecordObject(body=logs['logline'], attributes=logs[['cluster_label']])\n",
    "    similarity = pd.DataFrame()\n",
    "    custom_delimeter_regex = [r\"`+|\\s+\"]\n",
    "    preprocessor = Preprocessor(PreprocessorConfig(custom_delimiters_regex=custom_delimeter_regex))\n",
    "    preprocessed_loglines = preprocessor.clean_log(logrecord.body)\n",
    "    index_groups = preprocessor.group_log_index(logrecord.attributes, by=['cluster_label'])\n",
    "\n",
    "\n",
    "    for i in index_groups.index:\n",
    "        cluster_label = index_groups['cluster_label'].iloc[i]\n",
    "        indices = index_groups['group_index'][i]\n",
    "        if index_groups['cluster_label'].iloc[i] == -1:\n",
    "            continue\n",
    "        if len(indices) == 1:\n",
    "            continue\n",
    "        loglines_in_group = preprocessed_loglines.iloc[indices]\n",
    "        parsed_result = parse_logs(loglines_in_group, cluster_label, lrt)\n",
    "\n",
    "        uniq_patterns = parsed_result['parsed_logline'].unique()\n",
    "        num_p = len(uniq_patterns)\n",
    "\n",
    "        if num_p > 1:\n",
    "            similarity_table, base_pattern = get_sim_table(parsed_result[\"parsed_logline\"], lrt, cluster_label)\n",
    "            similarity = similarity.append(similarity_table)\n",
    "            parsed_result['parsed_logline'] = base_pattern\n",
    "            parsed_result['parameter_list'] = parsed_result.apply(get_parameter_list, axis=1)\n",
    "\n",
    "        para_list = parsed_result['parameter_list'].to_list()\n",
    "\n",
    "    return similarity, parsed_result, base_pattern, para_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'custom_delimeter_regex'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/gm/zf03v70n4ndcrg17spcgnrk80000gq/T/ipykernel_1766/2040279622.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0mdimensions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdimensions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mcustom_delimeter_regex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcustom_delimeter_regex\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m     \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m )\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'custom_delimeter_regex'"
     ]
    }
   ],
   "source": [
    "# Data layer\n",
    "# Load log data and store all data in @LogRecordObject. Currently only implemented FileDataLoader.\n",
    "\n",
    "# Please change the filepath correspondingly.\n",
    "# I've put the ./data dir in .gitignore to avoid checking in data unexpectedly\n",
    "\n",
    "#File Configuration\n",
    "\n",
    "filepath = \"/Users/qcheng/workspace/gitsoma/logai/logai/data/default_data/dbscan_clustering_clean - dbscan_clustering_clean.csv\"\n",
    "log_type = 'csv'\n",
    "dimensions = {'attributes': ['cluster_label'],\n",
    "              'body': ['logline']}\n",
    "custom_delimeter_regex = [r\"`+|\\s+\"]\n",
    "\n",
    "file_config = DataLoaderConfig(\n",
    "    filepath=filepath,\n",
    "    log_type='csv',\n",
    "    dimensions=dimensions,\n",
    "    custom_delimeter_regex=custom_delimeter_regex,\n",
    "    header=0\n",
    ")\n",
    "\n",
    "dataloader = FileDataLoader(file_config)\n",
    "logrecord = dataloader.load_data()\n",
    "# Preprocess\n",
    "# Do customer rules to initially parse the loglines. Add custom delimeters in a regex\n",
    "# Group log records by any attributes. Return grouped log index so follow up process can handle them separately.\n",
    "\n",
    "preprocessor = Preprocessor(PreprocessorConfig(custom_delimiters_regex=custom_delimeter_regex))\n",
    "preprocessed_loglines = preprocessor.clean_log(logrecord.body)\n",
    "\n",
    "#bucket loglines into groups.\n",
    "index_groups = preprocessor.group_log_index(attributes=logrecord.attributes, by=['cluster_label'])\n",
    "\n",
    "# Information Extraction\n",
    "drain_config = DrainParams(sim_th=0.4,\n",
    "                           extra_delimiters=[])\n",
    "\n",
    "log_parser_config = LogParserConfig(parsing_algorithm='drain',\n",
    "                                    parsing_algo_params=drain_config)\n",
    "\n",
    "# to_parse = preprocessed_loglines['logline']\n",
    "\n",
    "# parser = LogParser(log_parser_config)\n",
    "# parsed_result = parser.parse(to_parse)\n",
    "# num_patterns = dict()\n",
    "#\n",
    "# tree_path = '/Users/qcheng/workspace/gitsoma/logai/logai/results/pattern_discovery/tree/'\n",
    "#\n",
    "#\n",
    "# for i in index_groups.index[:500]:\n",
    "#     indices = index_groups['group_index'][i]\n",
    "#     loglines_in_group = preprocessed_loglines.iloc[indices]\n",
    "#     parser = LogParser(log_parser_config)\n",
    "#     parsed_result = parser.parse(loglines_in_group.dropna()[dimensions['body'][0]])\n",
    "#     longest_log_length = max([len(l) for l in parsed_result['logline']])\n",
    "#     uniq_patterns = parsed_result['parsed_logline'].unique()\n",
    "#     num_p = len(uniq_patterns)\n",
    "#     longest_p_length = max([len(p.split(\" \")) for p in uniq_patterns])\n",
    "#     num_patterns[index_groups['cluster_label'].iloc[i]] = [num_p, longest_p_length, longest_log_length]\n",
    "\n",
    "    #write results to file\n",
    "    # parsed_result.to_csv('../results/pattern_discovery_for_cluster_{}.csv'.format(index_groups['cluster_label'][i]))\n",
    "\n",
    "    # write generated tree to file\n",
    "    # write_path = join(tree_path, '{}.txt'.format(index_groups['cluster_label'][i]))\n",
    "    #\n",
    "    # if parser.parser.clusters_counter > 1:\n",
    "    #     if not exists(dirname(write_path)):\n",
    "    #         try:\n",
    "    #             os.makedirs(dirname(write_path))\n",
    "    #         except OSError as exc: # Guard against race condition\n",
    "    #             if exc.errno != exc.errno.EEXIST:\n",
    "    #                 raise\n",
    "    #     with open(write_path, 'w+') as f:\n",
    "    #         parser.parser.print_tree(max_clusters=100, file=f)\n",
    "    #         f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/gm/zf03v70n4ndcrg17spcgnrk80000gq/T/ipykernel_1766/3671884296.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m         \u001B[0mloglines_in_group\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpreprocessed_loglines\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m         \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mLogParser\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_parser_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0mparsed_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloglines_in_group\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DIR = \"/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/\"\n",
    "\n",
    "Res_DIR = \"/Users/qcheng/workspace/gitsoma/logai/logai/results/pattern_discovery/\"\n",
    "\n",
    "files = os.listdir(DIR)\n",
    "\n",
    "log_names = [f.split('.')[0] for f in files]\n",
    "\n",
    "num_patterns = dict()\n",
    "\n",
    "similarity = pd.DataFrame()\n",
    "\n",
    "for name in log_names:\n",
    "    log, cluster_label = read_log(name)\n",
    "\n",
    "    logrecord = LogRecordObject(body=pd.Series(log, name='logline'), attributes=pd.DataFrame(cluster_label, columns=['cluster_label']))\n",
    "\n",
    "    custom_delimeter_regex = [r\"`+|\\s+\"]\n",
    "\n",
    "    preprocessor = Preprocessor(PreprocessorConfig(custom_delimiters_regex=custom_delimeter_regex))\n",
    "    preprocessed_loglines = preprocessor.clean_log(logrecord.body)\n",
    "    index_groups = preprocessor.group_log_index(logrecord.attributes, by=['cluster_label'])\n",
    "    drain_config = DrainParams(sim_th=0.1,\n",
    "                               extra_delimiters=[])\n",
    "\n",
    "    log_parser_config = LogParserConfig(parsing_algorithm='drain',\n",
    "                                        parsing_algo_params=drain_config)\n",
    "\n",
    "    for i in index_groups.index:\n",
    "        cluster_label = index_groups['cluster_label'].iloc[i]\n",
    "        indices = index_groups['group_index'][i]\n",
    "        if index_groups['cluster_label'].iloc[i] == -1:\n",
    "            continue\n",
    "        if len(indices) == 1:\n",
    "            continue\n",
    "        loglines_in_group = preprocessed_loglines.iloc[indices]\n",
    "        parser = LogParser(log_parser_config)\n",
    "        parsed_result = parser.parse(loglines_in_group.dropna())\n",
    "        longest_log_length = max([len(l) for l in parsed_result['logline']])\n",
    "        uniq_patterns = parsed_result['parsed_logline'].unique()\n",
    "        num_p = len(uniq_patterns)\n",
    "        longest_p_length = max([len(p.split(\" \")) for p in uniq_patterns])\n",
    "        num_patterns[\"{}:{}\".format(name, index_groups['cluster_label'].iloc[i])] = [name, index_groups['cluster_label'].iloc[i], parsed_result.shape[0], num_p, longest_p_length, longest_log_length]\n",
    "\n",
    "        if num_p > 1:\n",
    "            f_path = os.path.join(Res_DIR, \"multiple_pattern_clusters_pattern_level\")\n",
    "            if not os.path.exists(f_path):\n",
    "                os.makedirs(f_path)\n",
    "            p_path = os.path.join(Res_DIR, \"token_simlarity\")\n",
    "            if not os.path.exists(p_path):\n",
    "                os.makedirs(p_path)\n",
    "\n",
    "            for pid in range(len(uniq_patterns)):\n",
    "                res_df = parsed_result[parsed_result['parsed_logline'] == uniq_patterns[pid]]\n",
    "                res = res_df['logline'].append(res_df.head(1)['parsed_logline'])\n",
    "                res.str.split(\" \", expand = True).to_csv(os.path.join(f_path, \"lrt_{}_cluster_{}_pattern_{}.csv\".format(name, index_groups['cluster_label'].iloc[i], pid)), index=False, header=False)\n",
    "            #parsed_result.to_csv(os.path.join(f_path, \"lrt_{}_cluster_{}.csv\".format(name, index_groups['cluster_label'].iloc[i])))\n",
    "\n",
    "            similarity_table = get_sim_table(parsed_result[\"parsed_logline\"], name, cluster_label)\n",
    "            similarity = similarity.append(similarity_table)\n",
    "\n",
    "            # similarity_table.to_csv(os.path.join(p_path, \"similarity_ltr_{}_cluster_{}.csv\".format(name, index_groups['cluster_label'].iloc[i])))\n",
    "\n",
    "        else:\n",
    "            f_path = os.path.join(Res_DIR, \"single_pattern_clusters_pattern_level\")\n",
    "            if not os.path.exists(f_path):\n",
    "                os.makedirs(f_path)\n",
    "            for pid in range(len(uniq_patterns)):\n",
    "                res_df = parsed_result[parsed_result['parsed_logline'] == uniq_patterns[pid]]\n",
    "                res = res_df['logline'].append(res_df.head(1)['parsed_logline'])\n",
    "                # res.str.split(\" \", expand = True).to_csv(os.path.join(f_path, \"lrt_{}_cluster_{}_pattern_{}.csv\".format(name, index_groups['cluster_label'].iloc[i], pid)), index=False, header=False)\n",
    "\n",
    "\n",
    "            #parsed_result.to_csv(os.path.join(f_path, \"lrt_{}_cluster_{}.csv\".format(name, index_groups['cluster_label'].iloc[i])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#similarity.to_csv(os.path.join(Res_DIR, \"similarity.csv\"))\n",
    "similarity.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get patterns\n",
    "\n",
    "# df = pd.DataFrame.from_dict(num_patterns, orient=\"index\")\n",
    "# df.columns=[\"log_record_type\", \"cluster_labels\", \"n_logline\", \"n_patterns\", \"longest_p_length\", \"longest_log_length\"]\n",
    "# count_table = df.n_patterns.groupby(df['log_record_type']).value_counts().rename(\"cluster_counts\")\n",
    "# count_total = df.n_patterns.groupby(df['log_record_type']).count().rename(\"total_clusters\")\n",
    "# count_logline = df.n_logline.groupby(df['log_record_type']).sum('n_logline').rename('logline_counts')\n",
    "# count_df = pd.DataFrame(count_table)\n",
    "#\n",
    "# total = count_df.join(pd.DataFrame(count_total).join(pd.DataFrame(count_logline)))\n",
    "# total['n_patterns'] =[i[1] for i in total.index]\n",
    "# total['log_record_type'] = [i[0] for i in total.index]\n",
    "#\n",
    "# total[\"ratio\"] = total['cluster_counts'] / total['total_clusters']\n",
    "# total.to_csv(\"/Users/qcheng/workspace/gitsoma/logai/logai/results/pattern_discovery/pattern_stats_exclude_single.csv\")\n",
    "\n",
    "# single_pattern = total.loc[(total[\"n_patterns\"] == 1)].drop([\"n_patterns\", \"log_record_type\"], axis=1)\n",
    "# summary = single_pattern.append(single_pattern.sum().rename((\"Total\", 1)))\n",
    "# summary[\"ratio\"] = summary['cluster_counts'] / summary['total_clusters']\n",
    "# summary.to_csv(\"/Users/qcheng/workspace/gitsoma/logai/logai/results/pattern_discovery/single_pattern_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count, bin = np.histogram(similarity[similarity['similarity'] < 1.0]['similarity'], bins=10, range=[0,1])\n",
    "ratio = count / sum(count)\n",
    "print(ratio)\n",
    "\n",
    "ratio[-1]+ratio[-2]+ratio[-3]+ratio[-4]+ratio[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "similarity[similarity['similarity'] < 1.0]['similarity'].plot.hist(bins=50, range=[0,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity[similarity['similarity'] < 0.6].to_csv(os.path.join(Res_DIR, \"less_similar.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "less_df = similarity[similarity['similarity'] < 0.6]\n",
    "\n",
    "f_path = os.path.join(Res_DIR, \"most_unsimilar_patterns\")\n",
    "if not os.path.exists(f_path):\n",
    "    os.makedirs(f_path)\n",
    "\n",
    "for ind, row in less_df.iterrows():\n",
    "    p_token = row['pattern'].split(\" \")\n",
    "    b_token = row['base_pattern'].split(\" \")\n",
    "    fname = \"{}_p_{}.csv\".format(row['cluster'], ind)\n",
    "    wfp = os.path.join(f_path, fname)\n",
    "    with open(wfp, \"w\") as f:\n",
    "        f.write(\",\".join(b_token))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\",\".join(p_token))\n",
    "\n",
    "        f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base = \"* * 0 0 * * * 232.3.3 INFO X *\"\n",
    "p = \"* * 0 0 * * * 232.3.3 INFO X *\"\n",
    "\n",
    "calc_distance(base, p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity[similarity['lrt'].str.contains('ailtn')]['similarity'].plot.hist(bins=50, range=[0,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity.loc[(similarity['lrt'].str.contains('ailtn')) & (similarity['similarity'] < 0.8)]['similarity']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity.groupby(by=['lrt']).mean()[['distance', 'similarity']].to_csv(os.path.join(Res_DIR, \"similarity_by_lrt.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity.groupby(by=['lrt']).mean()[['distance', 'similarity']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DIR = \"/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets_new_labels/\"\n",
    "\n",
    "file_list = os.listdir(DIR)\n",
    "\n",
    "similarity = pd.DataFrame()\n",
    "parsing_res = pd.DataFrame()\n",
    "\n",
    "for file in file_list[:1]:\n",
    "    path = os.path.join(DIR, file)\n",
    "    name = file.split('_')[0]\n",
    "    print(name)\n",
    "    print(path)\n",
    "    logs = read_new_logs(path)\n",
    "    sim, res, base_pattern, para_list = cal_similarity_for_lrt(logs, name)\n",
    "    similarity = similarity.append(sim)\n",
    "    parsing_res = parsing_res.append(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsing_res.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res['parameter_list'].to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}