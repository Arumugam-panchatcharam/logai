{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, exists, dirname\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.dataloader.data_loader import FileConfig\n",
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainConfig\n",
    "from logai.preprocess.preprocess import Preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Data layer\n",
    "# Load log data and store all data in @LogRecordObject. Currently only implemented FileDataLoader.\n",
    "\n",
    "# Please change the filepath correspondingly.\n",
    "# I've put the ./data dir in .gitignore to avoid checking in data unexpectedly\n",
    "\n",
    "#File Configuration\n",
    "filepath = \"/Users/qcheng/workspace/gitsoma/logai/logai/data/sf_datasets/mixed.csv\"\n",
    "log_type = 'csv'\n",
    "dimensions = {'attributes': ['cluster_label', 'logRecordType'],\n",
    "              'body': ['_raw']}\n",
    "custom_delimeter_regex = r\"`+|\\s+\"\n",
    "\n",
    "file_config = FileConfig(\n",
    "    filepath=filepath,\n",
    "    log_type='csv',\n",
    "    dimensions=dimensions,\n",
    "    custom_delimeter_regex=custom_delimeter_regex,\n",
    "    header=0\n",
    ")\n",
    "\n",
    "dataloader = FileDataLoader(file_config)\n",
    "logrecord = dataloader.load_data()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "# Do customer rules to initially parse the loglines. Add custom delimeters in a regex\n",
    "# Group log records by any attributes. Return grouped log index so follow up process can handle them separately.\n",
    "\n",
    "preprocessed_loglines = Preprocess.clean_log(logrecord.body, file_config.custom_delimeter_regex)\n",
    "\n",
    "#bucket loglines into groups.\n",
    "index_groups = Preprocess.group_log_index(logrecord.attributes, by=['logRecordType'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/qcheng/workspace/gitsoma/logai/logai/results/drain_cluster/tree/\n",
      "/Users/qcheng/workspace/gitsoma/logai/logai/results/drain_cluster/parsed_results/\n"
     ]
    }
   ],
   "source": [
    "# Information Extraction\n",
    "root_path = '/Users/qcheng/workspace/gitsoma/logai/logai/results/drain_cluster/'\n",
    "\n",
    "drain_config = DrainConfig(sim_th=0.9,\n",
    "                           extra_delimiters=[])\n",
    "\n",
    "log_parser_config = LogParserConfig(parsing_algorithm='drain',\n",
    "                                    parsing_algo_params=drain_config)\n",
    "\n",
    "num_patterns = dict()\n",
    "\n",
    "tree_path = os.path.join(root_path, \"tree/\")\n",
    "\n",
    "res_path = os.path.join(root_path, \"parsed_results/\")\n",
    "print(tree_path)\n",
    "print(res_path)\n",
    "\n",
    "if not exists(dirname(tree_path)):\n",
    "    os.makedirs(dirname(tree_path))\n",
    "\n",
    "if not exists(dirname(res_path)):\n",
    "    os.makedirs(dirname(res_path))\n",
    "\n",
    "for i in index_groups.index[:500]:\n",
    "    indices = index_groups['group_index'][i]\n",
    "    loglines_in_group = preprocessed_loglines.iloc[indices]\n",
    "    cluster_labels = logrecord.attributes['cluster_label'].iloc[indices]\n",
    "    parser = LogParser(log_parser_config)\n",
    "    parsed_result = parser.parse(loglines_in_group.dropna()[dimensions['body'][0]])\n",
    "    #parsed_result = pd.join((parsed_result, cluster_labels))\n",
    "    longest_log_length = max([len(l) for l in parsed_result['logline']])\n",
    "    uniq_patterns = parsed_result['parsed_logline'].unique()\n",
    "    num_uniq_cluster = len(cluster_labels.unique())\n",
    "    num_p = len(uniq_patterns)\n",
    "    longest_p_length = max([len(p.split(\" \")) for p in uniq_patterns])\n",
    "    num_patterns[index_groups['logRecordType'].iloc[i]] = [num_uniq_cluster, num_p, longest_p_length, longest_log_length]\n",
    "\n",
    "    #write results to file\n",
    "    pd.concat((parsed_result, cluster_labels), axis=1).to_csv(os.path.join(res_path, 'drain_cluster_{}.csv'.format(index_groups['logRecordType'][i])))\n",
    "\n",
    "    # write generated tree to file\n",
    "    write_path = join(tree_path, '{}.txt'.format(index_groups['logRecordType'][i]))\n",
    "\n",
    "    if parser.parser.clusters_counter > 1:\n",
    "        if not exists(dirname(write_path)):\n",
    "            try:\n",
    "                os.makedirs(dirname(write_path))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != exc.errno.EEXIST:\n",
    "                    raise\n",
    "        with open(write_path, 'w+') as f:\n",
    "            parser.parser.print_tree(max_clusters=100, file=f)\n",
    "            f.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame.from_dict(num_patterns, orient='index', columns=[\"num_clusters\", \"num_patterns\", \"longest_pattern_length\", \"longest_logline_length\"])\n",
    "res_df.to_csv(os.path.join(root_path, \"clustering_summary.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(86935, 1)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_loglines.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}